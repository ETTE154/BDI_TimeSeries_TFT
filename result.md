# 결과 분석 방법

## Attention 매커니즘
  - TFT (Temporal Fusion Transformer) 모델의 어텐션 메커니즘에서 사용되는 쿼리(Query), 키(Key), 밸류(Value)는 다음과 같이 연산됩니다.어텐션 메커니즘은 주어진 입력 시퀀스에 대해 다른 위치의 정보를 가중치를 부여하여 정보를 종합하는 과정입니다. 쿼리(Query), 키(Key), 밸류(Value)는 이 과정에서 핵심적인 역할을 수행합니다.

쿼리(Query), 키(Key), 밸류(Value) 계산:

**1. 먼저, 인코더 및 디코더의 각 위치에서 쿼리(Query), 키(Key), 밸류(Value)를 계산합니다. 이를 위해, 입력 벡터(인코더 및 디코더의 입력)에 대해 선형 변환을 수행합니다.**

Q = W_q * X
K = W_k * X
V = W_v * X

여기서,

Q, K, V는 각각 **쿼리(Query), 키(Key), 밸류(Value)** 행렬입니다.
W_q, W_k, W_v는 각각 **쿼리(Query), 키(Key), 밸류(Value)** 를 계산하기 위한 가중치 행렬입니다.
X는 인코더 또는 디코더의 입력 행렬입니다.
어텐션 가중치 계산:

**2. 쿼리(Query)와 키(Key) 행렬의 내적을 통해 각 위치 간의 유사도를 계산합니다. 이후, 소프트맥스(softmax) 함수를 적용하여 어텐션 가중치를 계산합니다.**

Attention_weights = softmax(Q * K^T / sqrt(d_k))

여기서,

d_k는 키(Key) 및 쿼리(Query) 벡터의 차원입니다.
"^T"는 전치(transpose)를 의미합니다.
sqrt는 제곱근을 의미합니다.
어텐션 출력 계산:

**3. 어텐션 가중치와 밸류(Value) 행렬의 곱을 통해 어텐션 출력을 계산합니다.**
Attention_output = Attention_weights * V

어텐션 출력은 다음 계층으로 전달되거나, 여러 개의 어텐션 헤드가 있는 경우 이를 결합하여 최종 출력을 생성합니다.
이렇게 계산된 어텐션 메커니즘은 TFT 모델의 인코더와 디코더에서 입력 시퀀스의 정보를 종합하는 데 사용되어, 시계열 데이터의 다양한 패턴을 포착하고 예측 성능을 향상시킵니다.

## plot_prediction
  - plot_prediction 함수는 실제 값과 예측 그리고 어텐션을 시각화합니다.

#### 매개변수:
  - **x** (Dict[str, torch.Tensor]): 네트워크 입력
  - **out** (Dict[str, torch.Tensor]): 네트워크 출력
  - **idx** (int): 샘플 인덱스
  - **plot_attention** (bool): 보조 축에 어텐션을 그릴지 여부, 기본값은 True입니다.
  - **add_loss_to_title** (bool): 제목에 손실을 추가할지 여부, 기본값은 False입니다.
  - **show_future_observed** (bool): 미래 실제 값을 표시할지 여부, 기본값은 True입니다.
  - **ax**: 그림을 그릴 matplotlib 축
  - **반환 값**:
    - plt.Figure: matplotlib 그림 함수는 먼저 일반적인 예측을 그립니다. 그런 다음 plot_attention이 True이면, 보조 축에 어텐션을 추가합니다. 
      이를 위해 interpret_output 함수를 사용하여 out의 해석을 얻고, 그 결과를 사용하여 어텐션을 그립니다. 마지막으로 tight_layout() 함수를 호출하여 그림의 레이아웃을 조정합니다.

#### Attention
  - 어텐션(Attention)은 딥러닝 모델, 특히 시퀀스 처리 작업에서 중요한 정보에 초점을 맞추도록 도움을 주는 메커니즘입니다.
    어텐션은 모델이 입력 시퀀스의 각 부분에 가중치를 할당하여 중요한 부분에 높은 가중치를 부여하고 상대적으로 덜 중요한 부분에 낮은 가중치를 부여합니다. 
    이를 통해 모델이 더욱 효과적으로 학습하고 예측을 수행할 수 있습니다.

#### plot_prediction 함수에서 어텐션을 그리는 과정은 다음과 같습니다:

  1. **interpret_output** 함수를 사용하여 네트워크 출력인 out의 해석을 얻습니다. 이 함수는 출력에 대한 어텐션 가중치를 계산합니다.
  2. 어텐션 가중치를 그래프로 그리기 위해, 먼저 기본 축(ax)에 대한 보조 축(ax2)을 생성합니다. 이렇게 하면 어텐션 그래프가 예측 그래프와 겹치지 않고 독립적으로 그려집니다.
  3. 보조 축에 **"Attention"** 레이블을 붙입니다.
  4. 인코더 길이를 사용하여 x축 범위를 설정하고, 어텐션 가중치를 y축 값으로 사용하여 그래프를 그립니다. 그래프의 투명도는 0.2로 설정하고, 선 색상은 검정색으로 지정합니다.
  5. **tight_layout**() 함수를 호출하여 그림의 레이아웃을 조정합니다.

  결과적으로, 이 함수는 예측 그래프와 함께 어텐션 가중치를 시각화하여, 어떤 입력 부분이 모델에 의해 더 중요하게 여겨지는지 이해하는 데 도움이 됩니다.
  
### Variables importance
#### Variable Selection Network 구조
  - Temporal Fusion Transformer (TFT) 모델의 Variable Selection Network는 입력 변수의 가변적인 중요도를 자동으로 학습하는 구조입니다. 이 네트워크는 각 입력 변수에 대해 가중치를 할당하고, 이 가중치를 이용하여 예측에 얼마나 영향력 있는지를 결정합니다. 이를 통해 모델은 불필요한 변수를 걸러내고, 관련성이 높은 변수에 집중할 수 있습니다.

Variable Selection Network는 크게 두 가지 부분으로 구성됩니다.

  1. **입력 게이팅(Input Gating) 메커니즘**: 각 입력 변수에 대해 게이트 가중치를 계산합니다. 이 가중치는 변수의 중요도를 나타내며, 이 가중치에 따라 변수가 얼마나 예측에 기여하는지 결정됩니다. 입력 게이팅 메커니즘은 각 변수에 대한 스칼라 가중치를 학습하며, 이 가중치를 변수의 값에 곱합니다. 이를 통해 중요한 변수는 강조되고, 덜 중요한 변수는 약화됩니다.

  2. **Temporal Variable Selection (TVS) 레이어**: 입력 시계열의 다양한 변수를 처리하기 위해 설계된 고유한 구조입니다. TVS 레이어는 인코더와 디코더의 모든 스텝에서 각 변수에 대해 별도의 가중치를 학습합니다. 이를 통해 모델은 시간에 따라 입력 변수의 중요도가 어떻게 변하는지를 고려할 수 있습니다.

Variable Selection Network는 이러한 구성요소를 사용하여 각 입력 변수의 중요도를 자동으로 학습합니다. 이 과정을 통해 TFT 모델은 다양한 입력 변수와 시간에 따라 변하는 중요도를 효과적으로 처리하며, 예측 성능을 높이는 데 도움이 됩니다.

#### best_tft.plot_interpretation(interpretation)
  - best_tft.plot_interpretation(interpretation) 함수는 Temporal Fusion Transformer (TFT) 모델의 해석 결과를 시각화합니다. 이 결과는 주로 다음 네 가지 범주로 나눌 수 있습니다.

  1. **Attention**: 이 그래프는 모델의 어텐션 메커니즘에 대한 정보를 보여줍니다. 어텐션 메커니즘은 인코더와 디코더 사이의 정보 흐름을 조절하며, 시계열 데이터의 특정 시점에 집중하여 예측에 도움이 되는 패턴을 찾습니다. 이 그래프에서는 각 시점의 어텐션 가중치가 어떻게 분포하는지 확인할 수 있습니다.

  2. **Static Variables**: 이 그래프는 static 변수의 중요도를 보여줍니다. Static 변수는 시간에 따라 변하지 않는 변수로, 주로 데이터의 고유한 특성을 나타냅니다. 이 그래프에서는 각 static 변수의 중요도를 비교하여 어떤 변수가 예측에 더 큰 영향을 미치는지 파악할 수 있습니다.

  3. **Encoder Variables**: 이 그래프는 인코더에서 사용되는 시계열 변수의 중요도를 보여줍니다. 인코더 변수는 시계열 데이터의 과거 정보를 나타내며, 인코더에서 처리되어 디코더로 전달됩니다. 이 그래프를 통해 각 인코더 변수가 모델의 예측에 얼마나 기여하는지를 평가할 수 있습니다.

  4. **Decoder Variables**: 이 그래프는 디코더에서 사용되는 시계열 변수의 중요도를 보여줍니다. 디코더 변수는 미래의 정보를 나타내며, 디코더에서 처리되어 최종 예측을 생성하는 데 사용됩니다. 이 그래프를 통해 각 디코더 변수가 모델의 예측에 얼마나 기여하는지를 평가할 수 있습니다.


## Example

### Q : TFT 모델을 이용하여 BDI를 예측하였을때 Encoder Variables importance의 결과가 FED_variation, FED, month, SSEC 순으로 나왔다. 이때에 BDI를 예측하는 프로젝트의 발표자가 되었다 가정하고 결과를 해석하여라.
  - A : 이 프로젝트에서 우리는 Temporal Fusion Transformer (TFT) 모델을 사용하여 BDI (발틱 건화물 지수)를 예측하였습니다. 인코더 변수 중요도의 결과를 분석한 결과, FED_variation, FED, month, SSEC 순으로 중요한 변수로 나타났습니다.
    1. **FED_variation**: 이 변수는 미국 연방준비제도(FED)의 금리 변동을 나타냅니다. 금리 변동은 경제의 상황과 정책 변화를 반영하며, 글로벌 건화물 시장에 영향을 미칩니다. 모델이 이 변수를 가장 중요하게 여기는 것은, FED 금리의 변동이 BDI 예측에 중요한 역할을 한다는 것을 나타냅니다.
    2. **FED**: 이 변수는 현재의 미국 연방준비제도(FED)의 금리를 나타냅니다. 금리 수준은 경제 활동에 영향을 미칩니다. 금리가 높을 때, 경제 활동이 둔화되고 건화물 수요가 감소하는 경향이 있습니다. 이 변수의 중요도가 높은 것은, 현재의 금리 수준이 BDI 예측에 영향을 미친다는 것을 보여줍니다.
    3. **month**: 이 변수는 월별 시계열 정보를 나타냅니다. 건화물 수요와 운임은 계절적 변동성을 가질 수 있으므로, 월 정보를 고려하는 것이 BDI 예측에 도움이 됩니다. 이 변수의 중요도가 높은 것은, 계절적 패턴이 BDI 예측에 중요한 역할을 한다는 것을 나타냅니다.
    4. **SSEC**: 이 변수는 상하이 증권 거래소 지수(SSEC)를 나타냅니다. 중국 경제의 상황과 글로벌 건화물 시장에 영향을 미칩니다. 이 변수의 중요도가 높은 것은, 중국 경제의 상황이 BDI 예측에 영향을 미친다는 것을 보여줍니다.

  - 이를 바탕으로, 우리는 BDI를 예측하기 위해 금리 변동, 현재 금리, 계절적 패턴, 중국 경제 상황 등을 고려하는 것이 중요함을 알 수 있습니다. 이러한 인사이트는 미래 BDI 예측의 정확성을 높이기 위해 변수 선택 및특성 공학에 도움이 될 것입니다. 또한 이 결과를 바탕으로, 발표자는 BDI에 영향을 미치는 주요 요인들에 대해 더 깊이 이해할 수 있으며, 관련 산업 분야에서의 전략적 결정에 도움이 될 수 있습니다.

  - 예를 들어, 금리 변동과 현재 금리가 BDI 예측에 큰 영향을 미친다는 것을 알게 되면, 관련 기업들은 미국의 금리 정책 변화와 금융 시장 동향에 더욱 주의를 기울일 것입니다. 또한 계절적 패턴의 중요성을 인식함으로써, 기업들은 성수기와 비수기에 대한 전략을 개발할 수 있습니다. 마지막으로, 중국 경제 상황이 BDI 예측에 영향을 미치기 때문에, 건화물 시장 참가자들은 중국 경제의 흐름과 정책 변화에 대한 정보를 수집하고 분석할 필요가 있습니다.

  - 이러한 정보와 인사이트는 BDI 예측뿐만 아니라, 건화물 시장과 관련된 다양한 분야에서도 도움이 될 것입니다. 예를 들어, 선박 회사들은 선박 투자 및 운영 전략을 수립할 때 이러한 요인들을 고려할 수 있으며, 건화물 수출입 기업들은 물류 및 수송 비용에 대한 예측을 개선하기 위해 이러한 정보를 활용할 수 있습니다.

  - 요약하면, 이 프로젝트에서 BDI 예측에 사용된 TFT 모델은 인코더 변수 중요도 결과를 통해 BDI에 영향을 미치는 주요 요인들에 대한 인사이트를 제공합니다. 이러한 정보는 BDI 예측의 정확성을 높이는 데 도움이 되며, 관련 산업 분야에서의 전략적 결정에 기여할 수 있습니다.
